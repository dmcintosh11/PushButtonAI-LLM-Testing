This is a repo to assist in implementing and testing multiple different LLMs to get metrics on how fast inferencing is across different hardware devices to find the most optimal set up for scaling LLM server use.

Plan on using for:
-Mixtral Norm
-Mixtral 8b Quant
-Mixtral 4b Quant
-Mixtral Vllm
-Mixtral Vllm AWQ4

Might use threading, but unsure if that is feasible given model setup